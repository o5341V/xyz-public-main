// ------------------------------------------------------------------------------------
// 
//    Probability Distribution Histogram
//    Version 1.1.9
// 
// ------------------------------------------------------------------------------------
// 
// Change log:
// 
//    Version 1.0
//      - Initial release.
//    Version 1.0.1
//      - Fixed minor spelling errors in the tooltip about the "data transformation method" option.
//      - Fixed a bug with the trimming of the data.
//    Version 1.0.2
//      - No code changes. Corrected the chart image used for the publication.
//    Version 1.0.3
//      - Added the first 4 moments to the TradingView Data window.
//    Version 1.1.0
//      - Updated code to PineScript version 5.
//      - Added support for comparing to a uniform distribution and a Laplace distribution.
//      - Fixed a few bugs.
//      - Added min and max of the data set to the TradingView Data window.
//    Version 1.1.1
//      - Added the option to show which bin the current data point belongs to.
//    Version 1.1.2
//      - Made the highlighted bin highlight the entire column instead of just the empirical distribution.
//    Version 1.1.3
//      - Fixed a bug with the highlighted bin where it wouldn't properly disapear when the checkbox was unticked.
//      - Added absolute and squared transformations to the data transformation dropdown list.
//      - Removed the log checkbox and added options for that in the dropdown list for data transformation.
//    Version 1.1.4
//      - Changed from calculating only centralized sample moments to ("unbiased") centralized sample variance and standardized sample skewness and kurtosis.
//      - Kurtosis is now showing "excess" kurtosis.
//    Version 1.1.5
//      - Fixed a bug that made the indicator crash when the first few data points were 'na'.
//      - Made the default transformation "log %-change" again. Accidentally made it "%-change" in the 1.1.3 update.
//    Version 1.1.6
//      - Added exponent of raw data to the data transformation dropdown list.
//    Version 1.1.7
//      - Added the option to use a lookback period for the histogram data source.
//    Version 1.1.8
//      - Added the option to set a date/time range for the data set.
//      - Added the option to set a custom bin size.
//    Version 1.1.9
//      - Added the median and mean absolute deviation (the average absolute distance between the median and the data points) to the TradingView Data window.
//      - Added the 1st quartile, 3rd quartile and interquartile range to the TradingView Data window.
//      - Added the 5th quantile and 95th quantile for both the empirical and the theoretical distribution to the TradingView Data window.
// 
// ------------------------------------------------------------------------------------
// 
// Licensed under CC BY-NC.
// https://creativecommons.org/licenses/by-nc/4.0/
// 
// Copyright Â© Anton Berlin, 2022.
// 
// ------------------------------------------------------------------------------------

//@version=5

indicator(title = "Probability Distribution Histogram", shorttitle = "ProbDistHist", overlay = false, precision = 8)



// ------------------------------------------------------------------------------------
//    Constants
// ------------------------------------------------------------------------------------
//{

color CLR_TRANSP = color.rgb(0, 0, 0, 100)

float CUMULANTS = 4

int X_AXIS_LABEL_ROWS = 6
int Y_AXIS_LABEL_COLUMNS = 1
float COLUMN_WIDTH = 0

string DISTR_NONE = "None"
string DISTR_UNIFORM = "Uniform distribution"
string DISTR_NORM = "Normal distribution"
string DISTR_LAPLACE = "Laplace distribution"

//} Constants



// ------------------------------------------------------------------------------------
//    Input
// ------------------------------------------------------------------------------------
//{

// Groupings
string GRP_DATA = "Data"
string GRP_HIST = "Histogram"

// Data transform methods
string TRANSFORM_NONE = "Raw Data (No Transformation)"
string TRANSFORM_DIFF = "Differenced Data"
string TRANSFORM_PERC_CHANGE = "%-Change"
string TRANSFORM_EXP = "Exp of Raw Data"
string TRANSFORM_LOG = "Log of Raw Data"
string TRANSFORM_LOG_DIFF = "Log of Differenced Data"
string TRANSFORM_LOG_PERC_CHANGE = "Log of %-Change"
string TRANSFORM_ABS = "Absolute of Raw Data"
string TRANSFORM_ABS_DIFF = "Absolute of Differenced Data"
string TRANSFORM_ABS_PERC_CHANGE = "Absolute of %-Change"
string TRANSFORM_ABS_LOG = "Absolute of Log of Raw Data"
string TRANSFORM_ABS_LOG_DIFF = "Absolute of Log of Differenced Data"
string TRANSFORM_ABS_LOG_PERC_CHANGE = "Absolute of Log of %-Change"
string TRANSFORM_SQR = "Square of Raw Data"
string TRANSFORM_SQR_DIFF = "Square of Differenced Data"
string TRANSFORM_SQR_PERC_CHANGE = "Square of %-Change"
string TRANSFORM_SQR_LOG = "Square of Log of Raw Data"
string TRANSFORM_SQR_LOG_DIFF = "Square of Log of Differenced Data"
string TRANSFORM_SQR_LOG_PERC_CHANGE = "Square of Log of %-Change"

// Tooltips
string TT_DIFF_DATA = "To correctly plot the probability distribution of the data, the data must be stationary. " +
     "One may therefore need to transform the data as a first step before plotting its distribution. Differencing means to subtract the " +
     "previous value from each value. For price data, you can usually do this or convert to %-change. Both work well, " +
     "but for price data %-change form is preferred. This option is therefore chosen by default. If the data is already stationary, " +
     "one can set this to Raw Data instead, to leave the data as is."
string TT_TRIM = "This is usually not recommended. However, if there are obvious problems with the outliers in the data, " +
     "sometimes it can help to trim some of the data from the data set."
string TT_TOP_ROW_PRECISION = "This controls how precise the top row X-Axis labels are (in terms of decimal places). " +
     "If you can't see a difference in value between two or more bins, try increasing this value."
string TT_SHOW_CUR_DATA = "Check this box to highlight which bin the current data point belongs to."
string TT_LOOKBACK = "Set this to 0 to use all available chart data. If this is set to anything other than 0, " +
     "only that number of bars will be included in the histogram, counting backwards from the current point in time."
string TT_DATE_TIME_RANGE = "Check this box to only include data from a specified date/time range."
string TT_CUSTOM_BIN_WIDTH = "If this is bigger than 0, a custom bin width is used. " +
     "If this is set to 0, the bin width is calculated automatically."

// Input variables
float src = input.source(close, "Input Data to Plot", group = GRP_DATA)
string data_transform_method = input.string(TRANSFORM_LOG_PERC_CHANGE, "Data Transformation Method",
     options =  [TRANSFORM_NONE, TRANSFORM_DIFF, TRANSFORM_PERC_CHANGE, TRANSFORM_EXP,
                 TRANSFORM_LOG, TRANSFORM_LOG_DIFF, TRANSFORM_LOG_PERC_CHANGE,
                 TRANSFORM_ABS, TRANSFORM_ABS_DIFF, TRANSFORM_ABS_PERC_CHANGE,
                 TRANSFORM_ABS_LOG, TRANSFORM_ABS_LOG_DIFF, TRANSFORM_ABS_LOG_PERC_CHANGE,
                 TRANSFORM_SQR, TRANSFORM_SQR_DIFF, TRANSFORM_SQR_PERC_CHANGE,
                 TRANSFORM_SQR_LOG, TRANSFORM_SQR_LOG_DIFF, TRANSFORM_SQR_LOG_PERC_CHANGE], group = GRP_DATA, tooltip = TT_DIFF_DATA)
float trim_from_top = input.float(0, "% to Trim from the TOP of the Data [0 to 100]", minval = 0, maxval = 100, step = 0.1, group = GRP_DATA, tooltip = TT_TRIM) / 100
float trim_from_bottom = input.float(0, "% to Trim from the BOTTOM of the Data [0 to 100]", minval = 0, maxval = 100, step = 0.1, group = GRP_DATA, tooltip = TT_TRIM) / 100
int lookback_window = input.int(0, "Lookback Window [0 = All Chart Data]", minval = 0, group = GRP_DATA, tooltip = TT_LOOKBACK)
bool use_date_time_range = input.bool(false, "Only Use Data From Below Date/Time Range", group = GRP_DATA, tooltip = TT_DATE_TIME_RANGE)
int time_start = input.time(timestamp("2021-01-01 00:00:00"), "Start Date/Time", group = GRP_DATA)
int time_end = input.time(timestamp("2022-12-31 00:00:00"), "End Date/Time", group = GRP_DATA)

int bins = input.int(30, "Number of Bins [1 to 50]", minval = 1, maxval = 50, group = GRP_HIST)
float custom_bin_width = input.float(0, "Custom Bin Width", minval = 0, group = GRP_HIST, tooltip = TT_CUSTOM_BIN_WIDTH)
float rows = float(input.int(50, "Bar Resolution [1 to 50]", minval = 1, maxval = 50, group = GRP_HIST))
float row_height = input.float(60, "Height (% of Panel) [33 to 100]", minval = 33, maxval = 100, group = GRP_HIST) / rows
color clr_hist = input.color(color.white, "Base Color", group = GRP_HIST)
color clr_distr_bar = input.color(color.new(color.blue, transp = 10), "Empirical Distribution Bar Color", group = GRP_HIST)
string theoretical_distr = input.string(DISTR_NORM, "Theoretical Distribution for Comparison",
     options = [DISTR_NONE, DISTR_UNIFORM, DISTR_NORM, DISTR_LAPLACE])
color clr_theoretical_bar = input.color(color.rgb(63, 63, 63, transp = 10), "Theoretical Distribution Bar Color", group = GRP_HIST)
color clr_lbl_pos_num = input.color(color.rgb(155, 255, 135, transp = 0), "Top Row X-Axis Label Color - Positive Values", group = GRP_HIST)
color clr_lbl_neg_num = input.color(color.rgb(255, 155, 135, transp = 0), "Top Row X-Axis Label Color - Negative Values", group = GRP_HIST)
string font_size = input.string(size.tiny, "Font Size", options = [size.small, size.tiny, size.auto], group = GRP_HIST)
int top_row_precision = input.int(4, "Top Row X-Axis Label Precision [0 to 10]", minval = 0, maxval = 10, group = GRP_HIST, tooltip = TT_TOP_ROW_PRECISION)
bool highlight_cur_bin = input.bool(true, "Highlight Bin for Current Data Point?", group = GRP_HIST, tooltip = TT_SHOW_CUR_DATA)
color clr_highlight_cur_bin = input.color(color.new(color.yellow, transp = 10), "Highlighted Bin Color", group = GRP_HIST)

// Colors based on "Base Color" (clr_hist) above
color clr_unfilled = color.new(clr_hist, transp = 95)
color clr_label_title = color.new(clr_hist, transp = 33)
color clr_text_fg = color.new(clr_hist, transp = 0)
color clr_text_bg = color.new(clr_hist, transp = 90)

//} Input



// ------------------------------------------------------------------------------------
//    Declarations
// ------------------------------------------------------------------------------------
//{

// Table used to plot the histogram
var table tbl_hist = table.new(
     position = position.bottom_center,
     columns = bins + Y_AXIS_LABEL_COLUMNS,
     rows = int(rows) + X_AXIS_LABEL_ROWS,
     border_width = 0)

// Variable to keep count of the number of NA values in the data set
var int na_count = 0

// Arrays
var array<float> data_array = array.new<float>()         // The data set itself
var array<int> count_array = array.new<int>(bins, 0)     // Data points per bin
var array<float> sum_array = array.new<float>(bins, 0)   // Sum per bin
array<float> p_array = array.copy(sum_array)             // Probability per bin

// Min and max of the data set
var float data_min = na
var float data_max = na

// Array to calculate cumulants (each element is the sum of deviations to the power of k for that specific moment)
var array<float> cumulants = array.new_float(int(CUMULANTS) - 1, 0)  // For the cumulants (except the first, the mean)

// Various statistics of the data set
var float data_median = na
var float data_mad = na
var float data_q1 = na
var float data_q3 = na
var float data_iqr = na
var float data_q5 = na
var float data_q95 = na
var float data_q5_t = na
var float data_q95_t = na
var float data_mean = na
var float data_variance = na
var float data_stdev = na
var float data_skewness = na
var float data_kurtosis = na

// Width of the bins
var float bin_width = na

// Flag that is used to initiate the bins - will be set to false once the bins have been initiated
var bool init_bins = true

//} Declarations



// ------------------------------------------------------------------------------------
//    Functions
// ------------------------------------------------------------------------------------
//{

// ------------------------------------------------------------------------------------
//    Binary search. Returns the index of the 'array' element closest to the supplied 'value'.
//    
//    Parameters:
//       array   The array id of the array to search
//       value   The value to search for
//    
//    Originally coded by @MichelT: https://www.tradingview.com/script/61GmO1ro-Insertion-sort-and-binary-search/
//    This function code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/
// ------------------------------------------------------------------------------------

binary_search(array<float> array, float x) =>
//{
    int index = na
    int left = 0
    int right = array.size(array) - 1
    int _range = right - left
    
    float element = na
    
    while (_range > 0 and left <= right)
    //{
        index := math.floor((left + right) / 2)
        element := array.get(array, index)
        
        if (element < x)
            left += 1
        else if (element > x)
            right -= 1
        else
            break, 0
    //}
    
    index
//}

// ------------------------------------------------------------------------------------
//    Blend two colors and return the new color.
//    
//    Parameters:
//       clr_a   The first color to blend
//       clr_b   The second color to blend
//    
//    Original code from: http://www.java2s.com/Code/Java/2D-Graphics-GUI/Blendtwocolors.htm
// ------------------------------------------------------------------------------------

blend_colors(color clr_a, color clr_b) =>
//{
    float alpha_a = (100 - color.t(clr_a)) / 100
    float alpha_b = (100 - color.t(clr_b)) / 100
    float total_alpha = alpha_a + alpha_b
    
    float weight_a = alpha_a / total_alpha
    float weight_b = alpha_b / total_alpha
    
    int r = math.round(weight_a * color.r(clr_a) + weight_b * color.r(clr_b))
    int g = math.round(weight_a * color.g(clr_a) + weight_b * color.g(clr_b))
    int b = math.round(weight_a * color.b(clr_a) + weight_b * color.b(clr_b))
    int t = math.min(100, math.round(100 * (1 - math.max(alpha_a, alpha_b))))
    
    color.rgb(r, g, b, t)
//}

// ------------------------------------------------------------------------------------
//    Approximation of the error function.
//    
//    Parameters:
//       value   Input value for the error function
// ------------------------------------------------------------------------------------

erf(float value) =>
//{
    float a = 0.147
    float ax2 = a * value * value
    
    if (value == 0)
        0
    else if (value == 1)
        1
    else
        math.sign(value) * math.sqrt(1 - math.exp(-(value * value) * ((4 / math.pi + ax2) / (1 + ax2))))
//}

// ------------------------------------------------------------------------------------
//    Approximation of the inverse error function.
//    Reference http://people.maths.ox.ac.uk/~gilesm/files/gems_erfinv.pdf
//    
//    Parameters:
//       value   Input value for the complementary error function. If you let y = erf(x) then erfinv(y) = x.
// ------------------------------------------------------------------------------------

erfinv(float value) =>
//{
    float w = na
    float p = na
    float x = na
    
    w := -math.log((1 - value) * (1 + value))
    
    if (na(value))
        na
    else if (w < 5)
    //{
        w -= 2.5
        p := 2.81022636e-08
        p := 3.43273939e-07 + p * w
        p := -3.5233877e-06 + p * w
        p := -4.39150654e-06 + p * w
        p := 0.00021858087 + p * w
        p := -0.00125372503 + p * w
        p := -0.00417768164 + p * w
        p := 0.246640727 + p * w
        p := 1.50140941 + p * w
        
        p * value
    //}
    else
    //{
        w := math.sqrt(w) - 3
        p := -0.000200214257
        p := 0.000100950558 + p * w
        p := 0.00134934322 + p * w
        p := -0.00367342844 + p * w
        p := 0.00573950773 + p * w
        p := -0.0076224613 + p * w
        p := 0.00943887047 + p * w
        p := 1.00167406 + p * w
        p := 2.83297682 + p * w
        
        p * value
    //}
//}

// ------------------------------------------------------------------------------------
//    Uniform distribution probability density function. Returns the probability
//    density of a value from a uniform distribution with a given minimum and maximum
//    value.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       min     The smallest value of the data set
//       max     The largest value of the data set
// ------------------------------------------------------------------------------------

dunif(float value, float min, float max) =>
//{
    if (na(value) or na(min) or na(max) or max <= min or min > value or value > max)
        0
    else
        1 / (max - min)
//}

// ------------------------------------------------------------------------------------
//    Uniform cumulative distribution probability function. Returns the cumulative
//    probability of a value from a uniform distribution with a given minimum and
//    maximum value.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       min     The smallest value of the data set
//       max     The largest value of the data set
// ------------------------------------------------------------------------------------

punif(float value, float min, float max) =>
//{
    if (na(value) or na(min) or na(max) or max < min)
        na
    else if (value >= max)
        0
    else if (value <= min)
        1
    else
        (value - min) / (max - min)
//}

// ------------------------------------------------------------------------------------
//    Inverse uniform cumulative distribution probability function. Returns the
//    quantile of the given cumulative probability from a uniform distribution with a
//    given minimum and maximum value.
//    
//    Parameters:
//       prob    The probability that we want back to a quantile
//       min     The smallest value of the data set
//       max     The largest value of the data set
// ------------------------------------------------------------------------------------

qunif(float prob, float min, float max) =>
//{
    if (na(prob) or na(min) or na(max))
        na
	else if (prob < 0 or prob > 1)
        na
	else if (max < min)
        na
	else if (max == min)
        min
	else
        min + prob * (max - min)
//}

// ------------------------------------------------------------------------------------
//    Normal distribution probability density function. Returns the probability density
//    of a value from a normal distribution with a given mean and standard deviation.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       mean    The mean of the data set
//       stdev   The standard deviation of the data set
// ------------------------------------------------------------------------------------

dnorm(float value, float mean, float stdev) =>
//{
    float z = (value - mean) / stdev
    
    if (na(value) or na(mean) or na(stdev) or stdev <= 0 or math.abs(z) >= 2 * math.sqrt(10E11))
        na
    else
	    (1 / math.sqrt(2 * math.pi)) * math.exp(-(math.pow(z, 2) / 2))
//}

// ------------------------------------------------------------------------------------
//    Normal cumulative distribution function. Returns the cumulative probability
//    of a value from a normal distribution with a given mean and standard deviation.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       mean    The mean of the data set
//       stdev   The standard deviation of the data set
// ------------------------------------------------------------------------------------

pnorm(float value, float mean, float stdev) =>
//{
    if (na(value) or na(mean) or na(stdev) or stdev <= 0)
        na
    else
        0.5 * (1 + erf((value - mean) / (stdev * math.sqrt(2))))
//}

// ------------------------------------------------------------------------------------
//    Inverse normal cumulative distribution probability function. Returns the
//    quantile of the given cumulative probability from a normal distribution with a
//    given mean and standard deviation.
//    
//    Parameters:
//       prob    The probability that we want back to a quantile
//       mean    The mean of the data set
//       stdev   The standard deviation of the data set
// ------------------------------------------------------------------------------------

qnorm(float prob, float mean, float stdev) =>
//{
    if (na(prob) or na(mean) or na(stdev))
        na
	else if (prob < 0 or prob > 1)
        na
	else if (stdev <= 0)
        na
    else
	    mean + stdev * math.sqrt(2) * erfinv(2 * prob - 1)
//}

// ------------------------------------------------------------------------------------
//    Laplace distribution probability density function. Returns the probability
//    density of a value from a Laplace distribution with a given median and
//    mean absolute deviation.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       median  The median of the data set
//       mad     The mean absolute deviation of the data set
// ------------------------------------------------------------------------------------

dlaplace(float value, float median, float mad) =>
//{
    float z = math.abs(value - median) / mad
    
    if (na(value) or na(median) or na(mad) or mad <= 0)
        na
    else
        1 / (2 * mad) * math.exp(-z)
//}

// ------------------------------------------------------------------------------------
//    Laplace cumulative distribution function. Returns the cumulative probability
//    of a value from a Laplace distribution with a given median and mean absolute
//    deviation.
//    
//    Parameters:
//       value   The value whose probability density we want to return
//       median  The median of the data set
//       mad     The mean absolute deviation of the data set
// ------------------------------------------------------------------------------------

plaplace(float value, float median, float mad) =>
//{
    if (na(value) or na(median) or na(mad) or mad <= 0)
        na
    else if (value < median)
        0.5 * math.exp((value - median) / mad)
    else
        1 - 0.5 * math.exp(-((value - median) / mad))
//}

// ------------------------------------------------------------------------------------
//    Inverse Laplace cumulative distribution probability function. Returns the
//    quantile of the given cumulative probability from a Laplace distribution with a
//    given median and mean absolute deviation.
//    
//    Parameters:
//       prob    The probability that we want back to a quantile
//       mean    The mean of the data set
//       stdev   The standard deviation of the data set
// ------------------------------------------------------------------------------------

qlaplace(float prob, float median, float mad) =>
//{
	if (na(prob) or na(median) or na(mad) or mad <= 0)
        na
    else if (prob < 0 or prob > 1)
        na
	else
	    prob < 0.5 ? median + mad * math.log(2 * prob) : median - mad * math.log(2 - 2 * prob)
//}

// ------------------------------------------------------------------------------------
//    Set the properties and text for the X-axis label cells in the table.
//    
//    Parameters:
//       column        The column of the X-axis label
//       label_row     The row of the X-axis label (can be between 0 and (X_AXIS_LABEL_ROWS - 1))
//       label_text    The label text for the cell
//       text_halign   The horizontal alignment for the label text in the cell
//       text_clr      The color to use for the label text
// ------------------------------------------------------------------------------------

x_axis_label_cell(int column, int label_row, string label_text, string text_halign, color text_clr) =>
//{
    int row = math.max(int(rows), math.min(int(rows) + label_row, int(rows) + X_AXIS_LABEL_ROWS - 1))
    
    table.cell_set_text(table_id = tbl_hist, column = column, row = row, text = label_text)
    table.cell_set_text_color(table_id = tbl_hist, column = column, row = row, text_color = text_clr)
    table.cell_set_text_halign(table_id = tbl_hist, column = column, row = row, text_halign = text_halign)
    table.cell_set_bgcolor(table_id = tbl_hist, column = column, row = row, bgcolor = clr_text_bg)
    table.cell_set_height(table_id = tbl_hist, column = column, row = row, height = 0)
//}

//} Functions



// ------------------------------------------------------------------------------------
//    Main code
// ------------------------------------------------------------------------------------
//{

// Collect the data point, transform it to the desired form
float data = switch (data_transform_method)
//{
    TRANSFORM_DIFF                => src - src[1]
    TRANSFORM_PERC_CHANGE         => src / src[1] - 1
    TRANSFORM_EXP                 => math.exp(src)
    TRANSFORM_LOG                 => math.log(src)
    TRANSFORM_LOG_DIFF            => math.log(src - src[1])
    TRANSFORM_LOG_PERC_CHANGE     => math.log(src / src[1])
    TRANSFORM_ABS                 => math.abs(src)
    TRANSFORM_ABS_DIFF            => math.abs(src - src[1])
    TRANSFORM_ABS_PERC_CHANGE     => math.abs(src / src[1] - 1)
    TRANSFORM_SQR                 => math.pow(src, 2)
    TRANSFORM_SQR_DIFF            => math.pow(src - src[1], 2)
    TRANSFORM_SQR_PERC_CHANGE     => math.pow(src / src[1] - 1, 2)
    TRANSFORM_ABS_LOG             => math.abs(math.log(src))
    TRANSFORM_ABS_LOG_DIFF        => math.abs(math.log(src - src[1]))
    TRANSFORM_ABS_LOG_PERC_CHANGE => math.abs(math.log(src / src[1]))
    TRANSFORM_SQR_LOG             => math.pow(math.log(src), 2)
    TRANSFORM_SQR_LOG_DIFF        => math.pow(math.log(src - src[1]), 2)
    TRANSFORM_SQR_LOG_PERC_CHANGE => math.pow(math.log(src / src[1]), 2)
    =>                               src
//}

// Store the data point in the array, unless it's 'na'
bool pushed_data = false

if ((use_date_time_range and time <= time_end and time >= time_start) or not use_date_time_range)
//{
    if (not na(data))
    //{
        if (not barstate.islast)
        //{
            array.push(data_array, data)
            pushed_data := true
        //}
        else if (data <= data_max and data >= data_min)
        //{
            array.insert(data_array, binary_search(data_array, data), data)
            pushed_data := true
        //}
    //}
    // If the data was 'na', count it, so that we can see how many missing data points we found in the data
    else
        na_count += 1
//}

// Update the data size (number of data points in the data set)
int data_size = array.size(data_array)

// If a lookback window is used and the data contains too many bars, purge the oldest bar.
float removed_data_point = na

if (pushed_data and data_size > lookback_window and lookback_window != 0)
//{
    removed_data_point := array.shift(data_array)
    data_size -= 1
//}

// Calculate probabilities for each bin
int cur_bin = na

if (barstate.isfirst)
//{
    for row = 0 to int(rows) + X_AXIS_LABEL_ROWS - 1
    //{
        for column = 0 to Y_AXIS_LABEL_COLUMNS + bins - 1
        //{
            table.cell(
                 table_id = tbl_hist,
                 column = column,
                 row = row, width = COLUMN_WIDTH,
                 height = row_height,
                 text = "x",
                 text_color = CLR_TRANSP,
                 text_size = font_size,
                 bgcolor = clr_unfilled)
        //}
    //}
    
    // Set up the cells for the X-axis label titles (column 0)
    x_axis_label_cell(0, 0, data_transform_method == TRANSFORM_PERC_CHANGE ? "%-Change" : "Data", text.align_left, clr_label_title)
    x_axis_label_cell(0, 1, "Count", text.align_left, clr_label_title)
    x_axis_label_cell(0, 2, "Empirical PMF", text.align_left, clr_label_title)
    x_axis_label_cell(0, 3, "Empirical CDF", text.align_left, clr_label_title)
    x_axis_label_cell(0, 4, "Theoretical PDF", text.align_left, clr_label_title)
    x_axis_label_cell(0, 5, "Theoretical CDF", text.align_left, clr_label_title)
    
    // Temporary preparation of the X-axis label cells for the first bar on the chart
    // This state of the table will likely never be seen by the user, but should be there just in case
    for column = 1 to 1 + bins - 1
        for i = 0 to X_AXIS_LABEL_ROWS - 1
            x_axis_label_cell(column, i, str.tostring(column + 1), text.align_center, clr_text_fg)
//}
// If this is the latest bar of the chart (closed bar or live bar), update the histogram data
else if (barstate.islast and data_size > 0)
//{
    int bin = na
    int bin_removed = na
    
    // If this is the first time the script runs on the last bar, we go through all data points and do the heavy lifting
    // so that when we get new ticks, we only have to update the probability array ('p_array'), 'count_array' and 'sum_array'
    // instead of having to go through this entire process for every single new tick
    if (init_bins)
    //{
        // Reset the flag
        init_bins := false
        
        // Sort the data array in ascending order
        array.sort(data_array, order.ascending)
        
        // Trim data if the user specified it and it's not already trimmed
        if (trim_from_top > 0 or trim_from_bottom > 0)
        //{
            int elements_from_top = math.ceil(trim_from_top * data_size)
            int elements_from_bottom = math.ceil(trim_from_bottom * data_size)
        
            data_array := array.copy(array.slice(data_array, elements_from_bottom, data_size - elements_from_top - 1))
            data_size := array.size(data_array)
        //}
        
        // Collect values for 'data_min', 'data_max', 'bid_width'
        data_min := array.get(data_array, 0)
        data_max := array.get(data_array, data_size - 1)
        bin_width := custom_bin_width != 0 ? custom_bin_width : (data_max - data_min) / float(bins - 1)
        
        // Count how the data points are distributed between the bins and store the sums of the data for each bin
        float data_point = na
        
        for i = 0 to data_size - 1
        //{
            data_point := array.get(data_array, i)
            bin := math.min(math.max(0, math.floor((data_point - data_min) / bin_width)), bins - 1)
            
            array.set(count_array, bin, array.get(count_array, bin) + 1)
            array.set(sum_array, bin, array.get(sum_array, bin) + data_point)
        //}
        
        // Calculate the mean of the data set
        data_mean := array.avg(data_array)
        
        // Calculate cumulants
        float dev_pow_k = na
        
        for k_index = 0 to int(CUMULANTS) - 2
        //{
            array.set(cumulants, k_index, 0)
            
            for i = 0 to data_size - 1
            //{
                data_point := array.get(data_array, i)
                dev_pow_k := math.pow(data_point - data_mean, k_index + 2)
                array.set(cumulants, k_index, array.get(cumulants, k_index) + dev_pow_k)
            //}
        //}
        
        // Calculate which bin the current data point belongs to
        cur_bin := math.min(math.max(0, math.floor((data - data_min) / bin_width)), bins - 1)
    //}
    // If we're not updating the bins completely, simply update 'count_array' and 'sum_array', and calculate the cumulants
    else
    //{
        // Update min and max values
        data_min := math.min(data_min, data)
        data_max := math.min(data_max, data)
        
        // Calculate which bin to update (i.e. which index of the bin arrays to use)
        bin := math.min(math.max(0, math.floor((data - data_min) / bin_width)), bins - 1)
        cur_bin := bin
        
        // Update 'count_array' and 'sum_array'
        array.set(count_array, bin, array.get(count_array, bin) + 1)
        array.set(sum_array, bin, array.get(sum_array, bin) + data)

        // If we removed a data point, we need to handle that
        if (not na(removed_data_point))
        //{
            bin_removed := math.min(math.max(0, math.floor((removed_data_point - data_min) / bin_width)), bins - 1)

            array.set(count_array, bin_removed, array.get(count_array, bin) - 1)
            array.set(sum_array, bin_removed, array.get(count_array, bin) - removed_data_point)
        //}
        
        // Calculate the mean of the data set
        data_mean := array.sum(sum_array) / data_size
        
        // Update the cumulants
        float dev_pow_k = na
        
        for k_index = 0 to int(CUMULANTS) - 2
        //{
            dev_pow_k := math.pow(data - data_mean, k_index + 2)
            array.set(cumulants, k_index, array.get(cumulants, k_index) + dev_pow_k)

            // And handle the removed data point
            if (not na(removed_data_point))
            //{
                dev_pow_k := math.pow(removed_data_point - data_mean, k_index + 2)
                array.set(cumulants, k_index, array.get(cumulants, k_index) - dev_pow_k)
            //}
        //}
    //}
    
    // Calculate the median and mean absolute deviation (MAD)
    data_median := array.median(data_array)
    
    float sum = 0
    
    for i = 0 to data_size == 0 ? na : data_size - 1
        sum += math.abs(array.get(data_array, i) - data_median)
    
    data_mad := sum / math.max(1, data_size)

    // Calculate the 1st quartile, 3rd quartile and interquartile range
    data_q1 := array.percentile_linear_interpolation(data_array, 25)
    data_q3 := array.percentile_linear_interpolation(data_array, 75)
    data_iqr := data_q3 - data_q1

    // Calculate the 5th and 95th quantile
    data_q5 := array.percentile_linear_interpolation(data_array, 5)
    data_q95 := array.percentile_linear_interpolation(data_array, 95)

    // Calculate the (unbiased) variance of the data set
    data_variance := data_size < 2 ? na :
         array.get(cumulants, 0) / (data_size - 1)
    
    // Calculate the (unbiased) standard deviation of the data set
    data_stdev := math.sqrt(data_variance)
    
    // Calculate the (unbiased) skewness of the data set
    data_skewness := data_size < 3 ? na :
         math.sqrt(data_size * (data_size - 1)) / (data_size - 2) *
         (array.get(cumulants, 1) / data_size) / math.pow(data_stdev, 3)
    
    // Calculate the (unbiased) excess kurtosis of the data set
    data_kurtosis := data_size < 4 ? na :
         (data_size * (data_size + 1)) / ((data_size - 1) * (data_size - 2) * (data_size - 3)) *
         (array.get(cumulants, 2) / math.pow(data_variance, 2))
         - (3 * (math.pow(data_size - 1, 2) / ((data_size - 2) * (data_size - 3))))
    
    // Update the probabilities for each bin (all the elements in 'p_array')
    for i = 0 to bins - 1
        array.set(p_array, i, array.get(count_array, i) / data_size)
//}

//} Main code



// ------------------------------------------------------------------------------------
//    Plotting
// ------------------------------------------------------------------------------------
//{

float theoretical_parameter_a = na
float theoretical_parameter_b = na

switch (theoretical_distr)
//{
    DISTR_UNIFORM =>
        theoretical_parameter_a := data_min
        theoretical_parameter_b := data_max
    DISTR_NORM =>
        theoretical_parameter_a := data_mean
        theoretical_parameter_b := data_stdev
    DISTR_LAPLACE =>
        if (barstate.islast)
        //{
            theoretical_parameter_a := data_median
            theoretical_parameter_b := data_mad
        //}
//}

float standard_theoretical_max_p = na

// If this is the last bar of the chart, update the table cells that make up the histogram
if (barstate.islast)
//{
    float bin = na
    float bin_value = na
    float bin_pd_row = na
    float bin_theoretical_pd_row = na
    
    color cell_bg_clr = na
    
    float empirical_p = na  // Empirical probability
    float empirical_cp = 0  // Empirical cumulative probability
    
    float empirical_max_p = array.max(p_array)
    standard_theoretical_max_p := switch (theoretical_distr)
    //{
        DISTR_UNIFORM => dunif(theoretical_parameter_a, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
        DISTR_NORM    => dnorm(theoretical_parameter_a, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
        DISTR_LAPLACE => dlaplace(theoretical_parameter_a, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
        => empirical_max_p
    //}
    
    float theoretical_p = na
    float theoretical_cp = na
    
    int max_row_index = int(rows) - 1
    
    color clr_empirical = na
    color clr_theoretical = na
    color clr_bg = na
    
    // Go through each column
    for column = Y_AXIS_LABEL_COLUMNS to Y_AXIS_LABEL_COLUMNS + bins - 1
    //{
        bin := column - 1
        
        // Update the X-axis labels
        bin_value := (data_min + bin * bin_width + bin_width / 2) * (data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1)
        
        empirical_p := array.get(p_array, int(bin))
        empirical_cp += empirical_p
        
        x_axis_label_cell(column, 0, str.tostring(math.round(bin_value, top_row_precision)) + (data_transform_method == TRANSFORM_PERC_CHANGE ? "%" : ""), text.align_center, bin_value > 0 ? clr_lbl_pos_num : bin_value < 0 ? clr_lbl_neg_num : clr_text_fg)
        x_axis_label_cell(column, 1, str.tostring(array.get(count_array, int(bin))), text.align_center, clr_text_fg)
        x_axis_label_cell(column, 2, str.tostring(empirical_p * 100, "0.00") + "%", text.align_center, clr_text_fg)
        x_axis_label_cell(column, 3, str.tostring(math.round(empirical_cp * 100, 2) == 100 and bin < bins - 1 ? 99.99 : empirical_cp * 100, "0.00") + "%", text.align_center, clr_text_fg)
        
        bin_value /= data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1
        
        // If the box to show the normal distribution is ticked, calculate the normal distribution probability density
        // for the data set and fit it to the histogram
        switch (theoretical_distr)
        //{
            DISTR_UNIFORM =>
                theoretical_p := dunif(bin_value, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
                theoretical_cp := punif(bin_value, theoretical_parameter_a, theoretical_parameter_b)
                data_q5_t := qunif(0.05, theoretical_parameter_a, theoretical_parameter_b)
                data_q95_t := qunif(0.95, theoretical_parameter_a, theoretical_parameter_b)
                
                if (column == Y_AXIS_LABEL_COLUMNS + bins - 1)
                //{
                    theoretical_p := standard_theoretical_max_p
                    theoretical_cp := 1
                //}
            DISTR_NORM =>
                theoretical_p := dnorm(bin_value, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
                theoretical_cp := pnorm(bin_value, theoretical_parameter_a, theoretical_parameter_b)
                data_q5_t := qnorm(0.05, theoretical_parameter_a, theoretical_parameter_b)
                data_q95_t := qnorm(0.95, theoretical_parameter_a, theoretical_parameter_b)
            DISTR_LAPLACE =>
                theoretical_p := dlaplace(bin_value, theoretical_parameter_a, theoretical_parameter_b) * theoretical_parameter_b
                theoretical_cp := plaplace(bin_value, theoretical_parameter_a, theoretical_parameter_b)
                data_q5_t := qlaplace(0.05, theoretical_parameter_a, theoretical_parameter_b)
                data_q95_t := qlaplace(0.95, theoretical_parameter_a, theoretical_parameter_b)
            =>
                theoretical_p := 0
                theoretical_cp := 0
        //}
        
        x_axis_label_cell(column, 4, str.tostring(theoretical_p * 100, "0.00"), text.align_center, clr_text_fg)
        x_axis_label_cell(column, 5, str.tostring(math.round(theoretical_cp * 100, 2) == 100 and bin < bins - 1 ? 99.99 : theoretical_cp * 100, "0.00") + "%", text.align_center, clr_text_fg)
        
        // Plot part of the distribution (actual and, if applicable, normal) by filling the right cells with the right colors in this column
        bin_pd_row := math.min(rows, math.ceil(rows - empirical_p / empirical_max_p * rows))
        bin_theoretical_pd_row := math.min(rows, math.ceil(rows - theoretical_p / standard_theoretical_max_p * rows))
        
        for row = 0 to max_row_index by 1
        //{
            // If the current bin in the loop is the same bin as the latest (current) data point,
            // and if "Highlight Bin for Current Data Point" is ticked, set the color to the "Highlighted Bin Color",
            // otherwise, set it to the "Empirical Distribution Bar Color"
            if (highlight_cur_bin and bin == cur_bin)
            //{
                clr_bg := blend_colors(clr_unfilled, color.new(clr_highlight_cur_bin, transp = color.t(clr_unfilled) * 0.67))
                clr_empirical := clr_highlight_cur_bin
                clr_theoretical := blend_colors(clr_theoretical_bar, clr_highlight_cur_bin)
            //}
            else
            //{
                clr_bg := clr_unfilled
                clr_empirical := clr_distr_bar
                clr_theoretical := clr_theoretical_bar
            //}
            
            if (row >= bin_pd_row or row == max_row_index and empirical_p > 0)
            //{
                if (row >= bin_theoretical_pd_row or row == max_row_index and theoretical_p > 0)
                    cell_bg_clr := blend_colors(clr_theoretical, clr_empirical)
                else
                    cell_bg_clr := clr_empirical
            //}
            else
            //{
                if (row >= bin_theoretical_pd_row or row == max_row_index and theoretical_p > 0)
                    cell_bg_clr := clr_theoretical
                else
                    cell_bg_clr := clr_bg
            //}
            
            table.cell_set_bgcolor(table_id = tbl_hist, column = column, row = row, bgcolor = cell_bg_clr)
        //}
    //}
//}

//} Plotting



// ------------------------------------------------------------------------------------
//    Data Window
// ------------------------------------------------------------------------------------
//{

plot(data_size, title = "Data Points", display = display.data_window)
plot(na_count, title = "Missing Data Points", display = display.data_window)
plot(cur_bin + 1, title = "Current Bin", display = display.data_window)

plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_min, title = "Min", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_max, title = "Max", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_mad, title = "Mean Absolute Deviation", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q1, title = "1st Quartile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_median, title = "2nd Quartile (Median)", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q3, title = "3rd Quartile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_iqr, title = "Interquartile Range", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q5, title = "5th Empirical Quantile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q95, title = "95th Empirical Quantile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q5_t, title = "5th Theoretical Quantile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_q95_t, title = "95th Theoretical Quantile", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_mean, title = "Mean", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_variance, title = "Variance", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_stdev, title = "Standard Deviation", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_skewness, title = "Skewness", display = display.data_window)
plot((data_transform_method == TRANSFORM_PERC_CHANGE ? 100 : 1) * data_kurtosis, title = "Excess Kurtosis", display = display.data_window)

//} Data Window
